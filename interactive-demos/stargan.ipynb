{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StarGAN\n",
        "\n",
        "### Image-to-Image translation samples\n",
        "\n",
        "| Hair Color | Age | Hair Color + Gender |\n",
        "|:----:|:----:|:----:|\n",
        "|  ![black haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_black_haired_female.png \"black haired female\")  ![blond haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_blond_haired_female.png \"blond haired female\")<br>&ensp;&ensp;**Black Hair** to **Blond Hair**&ensp;&ensp; | ![black haired young male](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_young_man.png \"young male\")  ![aged male](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_aged_man.png \"aged male\") <br>&ensp;&ensp;**Young** to **Aged**&ensp;&ensp; | ![black haired male](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_black_haired_male.png \"black haired male\")  ![blond haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/sample_blond_haired_female2.png \"blond haired female\") <br>**Black Hair/Male** to **Blond Hair/Female**|\n",
        "\n",
        "### From one domain to multi-domains\n",
        "\n",
        "|  Source<br>Black-Haired<br>Young Female |  Generated 1<br>Blond-Haired<br>Young Female  |  Generated 2<br>Brown-Haired<br>Young Female  |  Generated 3<br>Black-Haired<br>Young Male  |  Generated 4<br>Black-Haired<br>Aged Female |\n",
        "| ---- | ---- | ---- | ---- | ---- |\n",
        "|  ![black haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/source_Black_Hair_Young.png \"black haired young female\")  |  ![blond haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/generated_Blond_Hair_Young.png \"blond haired young female\")  |  ![brown haired female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/generated_Brown_Hair_Young.png \"brown haired young female\")  |  ![black haired male](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/generated_Black_Hair_Male_Young.png \"black haired young male\")  |  ![black haired aged female](https://github.com/sony/nnabla-examples/raw/master/GANs/stargan/imgs/generated_Black_Hair.png \"black haired aged female\")  |\n",
        "\n",
        "Here, we run [StarGAN](https://arxiv.org/abs/1711.09020), an Image-to-Image translation model based on [CycleGAN](https://arxiv.org/abs/1703.10593). Unlike CycleGAN, which translates images from one domain to another, StarGAN can translate from one domain to multiple domains with *one single model*. In this demo, we take a look at how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime â†’ change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%run nnabla-examples/interactive-demos/colab_utils.py\n",
        "%cd nnabla-examples/GANs/stargan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to download the pretrained weight and required config file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get StarGAN pretrained weights.\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/GANs/stargan/pretrained_params_on_celebA.h5\n",
        "\n",
        "# get StarGAN config file.\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/GANs/stargan/pretrained_conf_on_celebA.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload an image\n",
        "\n",
        "Run the below cell to upload an image to use. Make sure to select just 1 image (if you upload multiple images, all the images but the last one will be ignored) and that image must contain **one** face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "\n",
        "img = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience, rename the image file. Also, let's check the input image here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "ext = os.path.splitext(list(img.keys())[-1])[-1]\n",
        "os.rename(list(img.keys())[-1], \"input_image{}\".format(ext)) \n",
        "\n",
        "input_img = \"input_image\" + ext\n",
        "\n",
        "from IPython.display import Image,display\n",
        "display(Image(input_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the model expects the input images to contain a facial region only, We need to crop the image. This time we use dlib for face detection and cropping. First, we'll download the required dlib weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get dlib's face detection model.\n",
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "!bzip2 -d mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then import some dependency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from skimage import io, color\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now using dlib we'll detect the face in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = io.imread(input_img)\n",
        "if image.ndim == 2:\n",
        "    image = color.gray2rgb(image)\n",
        "elif image.shape[-1] == 4:\n",
        "    image = image[..., :3]\n",
        "\n",
        "face_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
        "detected_faces = face_detector(cv2.cvtColor(image[..., ::-1].copy(), cv2.COLOR_BGR2GRAY))\n",
        "detected_faces = [[d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()] for d in detected_faces]\n",
        "\n",
        "assert len(detected_faces) == 1, \"Warning: only one face should be contained.\"\n",
        "detected_faces = detected_faces[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, with some scripts, we extract the facial region only. These scripts are from [FAN example](https://github.com/sony/nnabla-examples/tree/master/facial-keypoint-detection/face-alignment), but partially modified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transform(point, center, scale, resolution, invert=False):\n",
        "    \"\"\"Generate and affine transformation matrix.\n",
        "    Given a set of points, a center, a scale and a target resolution, the\n",
        "    function generates and affine transformation matrix. If invert is ``True``\n",
        "    it will produce the inverse transformation.\n",
        "    Arguments:\n",
        "        point {numpy.array} -- the input 2D point\n",
        "        center {numpy.array} -- the center around which to perform the transformations\n",
        "        scale {float} -- the scale of the face/object\n",
        "        resolution {float} -- the output resolution\n",
        "    Keyword Arguments:\n",
        "        invert {bool} -- define wherever the function should produce the direct or the\n",
        "        inverse transformation matrix (default: {False})\n",
        "    \"\"\"\n",
        "    point.append(1)\n",
        "\n",
        "    h = 200.0 * scale\n",
        "    t = np.eye(3)\n",
        "    t[0, 0] = resolution / h\n",
        "    t[1, 1] = resolution / h\n",
        "    t[0, 2] = resolution * (-center[0] / h + 0.5)\n",
        "    t[1, 2] = resolution * (-center[1] / h + 0.5)\n",
        "\n",
        "    if invert:\n",
        "        t = np.reshape(np.linalg.inv(np.reshape(t, [1, 3, 3])), [3, 3])\n",
        "\n",
        "    new_point = np.reshape(np.matmul(\n",
        "        np.reshape(t, [1, 3, 3]), np.reshape(point, [1, 3, 1])), [3, ])[0:2]\n",
        "\n",
        "    return new_point.astype(int)\n",
        "\n",
        "\n",
        "def crop(image, center, scale, resolution=256):\n",
        "    \"\"\"Center crops an image or set of heatmaps\n",
        "    Arguments:\n",
        "        image {numpy.array} -- an rgb image\n",
        "        center {numpy.array} -- the center of the object, usually the same as of the bounding box\n",
        "        scale {float} -- scale of the face\n",
        "    Keyword Arguments:\n",
        "        resolution {float} -- the size of the output cropped image (default: {256.0})\n",
        "    Returns:\n",
        "        [type] -- [description]\n",
        "    \"\"\"  # Crop around the center point\n",
        "    \"\"\" Crops the image around the center. Input is expected to be an np.ndarray \"\"\"\n",
        "    ul = transform([1, 1], center, scale, resolution, True)\n",
        "    br = transform([resolution, resolution], center, scale, resolution, True)\n",
        "\n",
        "    if image.ndim > 2:\n",
        "        newDim = np.array([br[1] - ul[1], br[0] - ul[0],\n",
        "                           image.shape[2]], dtype=np.int32)\n",
        "        newImg = np.zeros(newDim, dtype=np.uint8)\n",
        "    else:\n",
        "        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)\n",
        "        newImg = np.zeros(newDim, dtype=np.uint8)\n",
        "    ht = image.shape[0]\n",
        "    wd = image.shape[1]\n",
        "    newX = np.array(\n",
        "        [max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)\n",
        "    newY = np.array(\n",
        "        [max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)\n",
        "    oldX = np.array(\n",
        "        [int(max(1, ul[0] + 1)), int(min(br[0], wd))], dtype=np.int32)\n",
        "    oldY = np.array(\n",
        "        [int(max(1, ul[1] + 1)), int(min(br[1], ht))], dtype=np.int32)\n",
        "\n",
        "    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1]\n",
        "           ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]\n",
        "\n",
        "    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)),\n",
        "                        interpolation=cv2.INTER_LINEAR)\n",
        "    return newImg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the scripts above, crop the uploaded image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "center = [detected_faces[2] - (detected_faces[2] - detected_faces[0]) / 2.0,\n",
        "          detected_faces[3] - (detected_faces[3] - detected_faces[1]) / 2.0]\n",
        "#center[1] = center[1] - (detected_faces[3] - detected_faces[1]) * 0.12\n",
        "scale = (detected_faces[2] - detected_faces[0] + detected_faces[3] - detected_faces[1]) / 195\n",
        "inp = crop(image, center, scale, resolution=128)\n",
        "plt.imshow(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, save this cropped image and move it to the new directory named \"source_img\". This image will be used as an input to StarGAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import shutil\n",
        "io.imsave(\"cropped_image.png\", inp)\n",
        "source_dir = \"source_img\"\n",
        "os.makedirs(source_dir, exist_ok=True)\n",
        "shutil.move(\"cropped_image.png\", f\"source_img/input_image.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now run StarGAN!\n",
        "Now that we prepared all the required files, let's run StarGAN. In this demo, 5 attributes, `Black Hair`, `Blond Hair`, `Brown Hair`, `Male` and `Young` can be chosen as *attributes*. \n",
        "\n",
        "For each attribute, you will be asked whether you want to *add* that attribute to the input image. You need to type `yes` or `no`.\n",
        "\n",
        "\n",
        "For instance, first you will be asked whether or not to `use 'Black_Hair'`. Then if you type `yes`, the model will try to change the input's hair color to black (note that the model doesn't consider the original image's hair color, so it can be possible that the black haired person's hair can be modified even though it is already black). \n",
        "\n",
        "\n",
        "If you type `no`, that attribute will not be added. \n",
        "\n",
        "\n",
        "As for 4th and 5th attributes, `Male` and `Young`, if you don't want to use these attributes, the model will add the *opposite* attribute, in other words, `female` and `aged`.\n",
        "\n",
        "\n",
        "Seeing is believing, just give it a shot!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python generate.py --pretrained-params pretrained_params_on_celebA.h5 --config pretrained_conf_on_celebA.json --test-image-path source_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob\n",
        "generated_img = sorted(glob.glob(os.path.join(\"tmp.results/*.png\")), key=os.path.getmtime)[-1]\n",
        "#print(generated_img)\n",
        "display(Image(generated_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try with your face\n",
        "Now, if your machine has a webcam, Colab will access to that webcam and you can capture your own image. Similar to the previous demo, with some preprocessing such as face detection and cropping, you can try StarGAN on your face! Let's see what happens.\n",
        "Executing the following cell will enable the camera. Just press `Capture` and captured image will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "    filename = take_photo(cam_width=256, cam_height=256)\n",
        "    print('Saved to {}'.format(filename))\n",
        "    # Show the image which was just taken.\n",
        "    display(Image(filename))\n",
        "except Exception as err:\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "    # grant the page permission to access it.\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following cell, the captured image will be cropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = io.imread(\"photo.png\")\n",
        "if image.ndim == 2:\n",
        "    image = color.gray2rgb(image)\n",
        "elif image.shape[-1] == 4:\n",
        "    image = image[..., :3]\n",
        "\n",
        "face_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
        "detected_faces = face_detector(cv2.cvtColor(image[..., ::-1].copy(), cv2.COLOR_BGR2GRAY))\n",
        "detected_faces = [[d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()] for d in detected_faces]\n",
        "\n",
        "assert len(detected_faces) == 1, \"Warning: only one face should be contained.\"\n",
        "detected_faces = detected_faces[0]\n",
        "\n",
        "center = [detected_faces[2] - (detected_faces[2] - detected_faces[0]) / 2.0,\n",
        "          detected_faces[3] - (detected_faces[3] - detected_faces[1]) / 2.0]\n",
        "#center[1] = center[1] - (detected_faces[3] - detected_faces[1]) * 0.12\n",
        "scale = (detected_faces[2] - detected_faces[0] + detected_faces[3] - detected_faces[1]) / 195\n",
        "inp = crop(image, center, scale, resolution=128)\n",
        "plt.imshow(inp)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "io.imsave(\"cropped_image.png\", inp)\n",
        "source_dir = \"source_img\"\n",
        "os.makedirs(source_dir, exist_ok=True)\n",
        "shutil.move(\"cropped_image.png\", f\"source_img/input_image.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the images shown above looks OK, just execute the following cell! It will run `StarGAN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python generate.py --pretrained-params pretrained_params_on_celebA.h5 --config pretrained_conf_on_celebA.json --test-image-path source_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "generated_img = sorted(glob.glob(os.path.join(\"tmp.results/*.png\")), key=os.path.getmtime)[-1]\n",
        "#print(generated_img)\n",
        "display(Image(generated_img))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
