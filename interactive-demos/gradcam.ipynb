{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "gradcam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKAbTFrlGF0O"
      },
      "source": [
        "# Grad-CAM\n",
        "This example interactively demonstrates Grad-CAM using nnabla's pre-trained model.\n",
        "\n",
        "Grad-CAM : Visual Explanation from Deep Networks via Gradient-based Localization<br>\n",
        "Ramprasaah R. Selvaraju et. al., arXiv (2017) <br>\n",
        "https://arxiv.org/abs/1610.02391"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij3zQo9cG8le"
      },
      "source": [
        "Where does machine learning look at images to make decisions? This paper describes Grad-CAM, an algorithm that uses heat maps to indicate the image areas used as the basis for judgment in image classification by CNN [R.R. Selvaraju + 2016]. The principle behind Grad-CAM is to visualize the information on which the decision is based by displaying the location of the large input gradient to the final convolution layer. Grad-CAM allows you to visualize areas of relevance for each prediction class. In the final output, the gradient is backpropagated, with only the specified class being 1 and the other classes being 0, to find the gradient in the feature map. Uses the average slope on each channel as a weight, and adds that weight to the values in the feature map. The Grad-CAM is computed through the activation function ReLU to remove the negative part. The feature is that the operation is very light, so it can be operated at high speed. As an example of application, we can see where the misclassified images were judged. It can also detect cases where misclassification occurs due to gender differences.\n",
        "\n",
        "![Grad-CAM algorithm](https://github.com/sony/nnabla-examples/raw/master/responsible_ai/gradcam/images/grad-cam_fig.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNZqVzzZGF0X"
      },
      "source": [
        "## Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSxlbQDfGF0Y"
      },
      "source": [
        "!pip install nnabla\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples/responsible_ai/gradcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKII4V6-GF0a"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R16x__BQGF0b"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nnabla as nn\n",
        "from nnabla.utils.image_utils import imread\n",
        "from nnabla.models.imagenet import VGG16\n",
        "\n",
        "from gradcam import gradcam\n",
        "from gradcam import overlay_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUKDyTPIGF0c"
      },
      "source": [
        "## Image Preparation \n",
        "Download image to apply Grad-CAM for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EYYpg1dGF0c"
      },
      "source": [
        "url = 'https://upload.wikimedia.org/wikipedia/commons/4/4e/A_crab_spider_on_a_flower_preying_upon_a_euglossine_bee%2C_while_a_butterfly_looks_for_nectar.jpg'\n",
        "img_path = 'input_flower_moth_spider.jpg'\n",
        "if not os.path.isfile(img_path):\n",
        "    tgt = urllib.request.urlopen(url).read()\n",
        "    with open(img_path, mode='wb') as f:\n",
        "        f.write(tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HLuqJf-GF0d"
      },
      "source": [
        "Take a look at what the image looks like.  \n",
        "We can see a flower in the middle on which a butterfly rests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ralmYCVTGF0d"
      },
      "source": [
        "img = imread(img_path, size=(224, 224), channel_first=True)\n",
        "plt.imshow(img.transpose(1,2,0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulz09HAXGF0e"
      },
      "source": [
        "## Network Definition\n",
        "Loading the model is very simple.<br>\n",
        "You can choose other models such as `VGG11`, `VGG13`, by specifying the model's name as an argument. Of course, you can choose other pretrained models as well. See the [Docs](https://nnabla.readthedocs.io/en/latest/python/api/models/imagenet.html).\n",
        "\n",
        "**NOTE**: If you use the `VGG16` for the first time, nnabla will automatically download the weights from `https://nnabla.org` and it may take up to a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Oty7lljEGF0f"
      },
      "source": [
        "model = VGG16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r240UI5lGF0g"
      },
      "source": [
        "batch_size = 1\n",
        "x = nn.Variable((batch_size,) + model.input_shape)\n",
        "# set training True since gradient of variable is necessary for Grad-CAM\n",
        "vgg = model(x, training=True, returns_net=True)\n",
        "vgg_variables = vgg.variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJ2mo1hGF0g"
      },
      "source": [
        "We now define the input, and extract the necessary outputs.  \n",
        "middle_layer: the last convolution layer  \n",
        "pred: final output of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF_Cv-8pGF0h"
      },
      "source": [
        "target_label_indices = {\n",
        "    'butterfly': 326,# lycaenid, lycaenid butterfly\n",
        "    'flower': 985,# daisy\n",
        "    'spider': 74,# garden spider\n",
        "}\n",
        "\n",
        "input_name = list(vgg.inputs.keys())[0]\n",
        "vgg_variables[input_name].d = img\n",
        "middle_layer = vgg_variables['VGG16/ReLU_13']\n",
        "pred = vgg_variables[\"VGG16/Affine_3\"]\n",
        "selected = pred[:, target_label_indices['butterfly']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy_Pgj_sGF0h"
      },
      "source": [
        "Let's see how the model predicted the image.  \n",
        "We can see the model classified the image as we expect.  \n",
        "Labels regarding butterfly comes high, while flower is also recognized although it is14th ranked probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqLuq4srGF0i"
      },
      "source": [
        "selected.forward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9yJ2uAGF0i"
      },
      "source": [
        "predicted_labels = np.argsort(-pred.d[0])\n",
        "for i, predicted_label in enumerate(predicted_labels[:15]):\n",
        "        print(f'Top {i+1}, Label index: {predicted_label},  Label name: {model.category_names[predicted_label]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QsAoeSLGF0i"
      },
      "source": [
        "## Grad-CAM Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsOft_PGF0j"
      },
      "source": [
        "Execute backward computation to calculate gradient to use for Grad-CAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieqp5iARGF0j"
      },
      "source": [
        "selected.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYsvM6e7GF0k"
      },
      "source": [
        "Let's compute the heatmap using the gradient with respect to the last convolution layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VimBZEQAGF0k"
      },
      "source": [
        "heatmap = gradcam(middle_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxMt28WUGF0l"
      },
      "source": [
        "## Visualization\n",
        "Take a look at how the heatmap looks like in the layer of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47FUNitSGF0l"
      },
      "source": [
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJvrf5naGF0m"
      },
      "source": [
        "Then we overlay the heatmap onto the original image to understand where the model focused."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTanVEd5GF0m"
      },
      "source": [
        "base_img = imread(img_path, size=(224, 224))\n",
        "overlaid_img_butterfly = overlay_images(base_img, heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq0vT4KCGF0n"
      },
      "source": [
        "Now we overlay the heatmap onto the original image to understand where the model focused.  \n",
        "We can speculate the model recognized the butterfly, focusing on its wing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2YIazBrGF0n"
      },
      "source": [
        "plt.imshow(overlaid_img_butterfly)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyAZv__QGF0n"
      },
      "source": [
        "Let's use flower as the label of interest to see how the model see the image this time by calculating Grad-CAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYpqUiN0GF0n"
      },
      "source": [
        "Reset gradient first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goSNDfB-GF0o"
      },
      "source": [
        "for k, v in vgg_variables.items():\n",
        "    if 'VGG16/' in k:\n",
        "        v.grad.zero()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giFj9uvYGF0o"
      },
      "source": [
        "Calculate gradient and Grad-CAM same as for butterfly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08DxXtEpGF0o"
      },
      "source": [
        "selected_daisy = pred[:, target_label_indices['flower']]\n",
        "selected_daisy.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stzLVKIfGF0o"
      },
      "source": [
        "heatmap_daisy = gradcam(middle_layer)\n",
        "plt.matshow(heatmap_daisy)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcmoZQpjGF0p"
      },
      "source": [
        "We can see the model focus is widely spread comparing to than for butterfly as if the heatmap wrapped the flower. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B17EwaNSGF0p"
      },
      "source": [
        "overlaid_img_daisy = overlay_images(base_img, heatmap_daisy)\n",
        "plt.imshow(overlaid_img_daisy)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4kixx3aGF0r"
      },
      "source": [
        "Finally, compare images in oneline to enable to see the differences clearly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUKg64sZGF0r"
      },
      "source": [
        "images = {\n",
        "    'original': base_img,\n",
        "    'butterfly': overlaid_img_butterfly,\n",
        "    'flower': overlaid_img_daisy,\n",
        "}\n",
        "\n",
        "\n",
        "row = 1\n",
        "col = len(images)\n",
        "fig, axes = plt.subplots(row, col, figsize=(15,15))\n",
        "\n",
        "for i, (k, v) in enumerate(images.items()):\n",
        "    axes[i].imshow(v)\n",
        "    axes[i].set_title(k)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}