{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2D and 3D Face Alignment Network\n",
        "**2D FAN:**\n",
        "\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/facial-keypoint-detection/face-alignment/results/example1.png)![alt text](https://github.com/sony/nnabla-examples/raw/master/facial-keypoint-detection/face-alignment/results/example2.png)\n",
        "\n",
        "**3D FAN:**\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/facial-keypoint-detection/face-alignment/results/example1_3d.png)![alt text](https://github.com/sony/nnabla-examples/raw/master/facial-keypoint-detection/face-alignment/results/example2_3d.png)\n",
        "\n",
        "Here we'll show you a facial landmark detection example using [FAN (Face Alignment Network)](https://arxiv.org/pdf/1703.07332.pdf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation\n",
        "\n",
        "First, we need to have [nnabla-examples repository](https://github.com/sony/nnabla-examples) and install nnabla. The following cell does both. Also, when you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime â†’ change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%run nnabla-examples/interactive-demos/colab_utils.py\n",
        "%cd nnabla-examples/facial-keypoint-detection/face-alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we need to download the pretrained weights and face detection model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get 2D FAN pretrained weights.\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/face-alignment/2DFAN4_NNabla_model.h5\n",
        "\n",
        "# to run the 3D facial landmark detection, we need to have additional weights as well.\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/face-alignment/3DFAN4_NNabla_model.h5\n",
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/face-alignment/Resnet_Depth_NNabla_model.h5\n",
        "\n",
        "# get dlib's face detection model.\n",
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Upload an image\n",
        "\n",
        "Run the below cell to upload an image to use FAN. Make sure to select **just 1 image** (if you upload multiple images, all the images but the last one will be ignored) and that image must contain at least one face."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import files\n",
        "\n",
        "img = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience, rename the image file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "ext = os.path.splitext(list(img.keys())[-1])[-1]\n",
        "os.rename(list(img.keys())[-1], \"input_image{}\".format(ext)) \n",
        "\n",
        "input_img = \"input_image\" + ext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run 2D FAN & Visualize the result\n",
        "\n",
        "Now that we have the image to use, let's run 2D FAN and see the result. The following cell executes FAN network and generates the output image named \"output_image.png\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python model_inference.py --model 2DFAN4_NNabla_model.h5 --test-image $input_img --output output_image.png\n",
        "\n",
        "from IPython.display import Image,display\n",
        "display(Image('output_image.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run 3D FAN & Visualize the result\n",
        "\n",
        "Next, let's see what happens if we use 3D landmark detection. The following cell executes 3D FAN and generate the output image named \"output_image_3D.png\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python model_inference.py --landmarks-type-3D --model 3DFAN4_NNabla_model.h5 --resnet-depth-model Resnet_Depth_NNabla_model.h5 --test-image $input_img --output output_image_3D.png\n",
        "\n",
        "display(Image('output_image_3D.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize the 3D FAN result with 3D plot\n",
        "\n",
        "The image above shows the keypoints, but since it is rendered on 2D image it's hard to tell the difference from the 2D result. So next we use mplot3d to see how the detected keypoints are represented in the 3D space. Note that the script below is a partially modified version of model_inference.py [as of ver 1.9.0](https://github.com/sony/nnabla-examples/blob/release/v1.9.0-branch/facial-keypoint-detection/face-alignment/model_inference.py), so there might be some difference in the later version.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      "cellView": "form"
      },
      "source": [
        "#@title First let's start by importing dependencies. (double-click to see the codes)\n",
        "import cv2\n",
        "import dlib\n",
        "import nnabla as nn\n",
        "import nnabla.functions as F\n",
        "from skimage import io, color\n",
        "from model import fan, resnet_depth\n",
        "from external_utils import *\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the main part of the keypoint detection. Using the uploaded images above, we first detect the face region with dlib, then crop that area to extract the facial image, and finally run the FAN to get the predicted keypoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      "cellView": "form"
      },
      "source": [
        "#@title Execute Face Detection and FAN. (double-click to see the codes)\n",
        "from nnabla.ext_utils import get_extension_context\n",
        "ctx = get_extension_context(\"cudnn\")\n",
        "nn.set_default_context(ctx)\n",
        "nn.set_auto_forward(True)\n",
        "\n",
        "image = io.imread(input_img)\n",
        "if image.ndim == 2:\n",
        "    image = color.gray2rgb(image)\n",
        "elif image.ndim == 4:\n",
        "    image = image[..., :3]\n",
        "\n",
        "face_detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")\n",
        "detected_faces = face_detector(cv2.cvtColor(image[..., ::-1].copy(), cv2.COLOR_BGR2GRAY))\n",
        "detected_faces = [[d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom()] for d in detected_faces]\n",
        "\n",
        "if len(detected_faces) == 0:\n",
        "    print(\"Warning: No faces were detected.\")\n",
        "    sys.exit()\n",
        "\n",
        "# Load FAN weights\n",
        "with nn.parameter_scope(\"FAN\"):\n",
        "    print(\"Loading FAN weights...\")\n",
        "    nn.load_parameters(\"3DFAN4_NNabla_model.h5\")\n",
        "\n",
        "# Load ResNetDepth weights\n",
        "with nn.parameter_scope(\"ResNetDepth\"):\n",
        "    print(\"Loading ResNetDepth weights...\")\n",
        "    nn.load_parameters(\"Resnet_Depth_NNabla_model.h5\")\n",
        "\n",
        "landmarks = []\n",
        "for i, d in enumerate(detected_faces):\n",
        "    center = [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0]\n",
        "    center[1] = center[1] - (d[3] - d[1]) * 0.12\n",
        "    scale = (d[2] - d[0] + d[3] - d[1]) / 195\n",
        "    inp = crop(image, center, scale)\n",
        "    inp = nn.Variable.from_numpy_array(inp.transpose((2, 0, 1)))\n",
        "    inp = F.reshape(F.mul_scalar(inp, 1 / 255.0), (1,) + inp.shape)\n",
        "    with nn.parameter_scope(\"FAN\"):\n",
        "        out = fan(inp, 4)[-1]\n",
        "    pts, pts_img = get_preds_fromhm(out, center, scale)\n",
        "    pts, pts_img = F.reshape(pts, (68, 2)) * \\\n",
        "        4, F.reshape(pts_img, (68, 2))\n",
        "\n",
        "    heatmaps = np.zeros((68, 256, 256), dtype=np.float32)\n",
        "    for i in range(68):\n",
        "        if pts.d[i, 0] > 0:\n",
        "            heatmaps[i] = draw_gaussian(\n",
        "                heatmaps[i], pts.d[i], 2)\n",
        "    heatmaps = nn.Variable.from_numpy_array(heatmaps)\n",
        "    heatmaps = F.reshape(heatmaps, (1,) + heatmaps.shape)\n",
        "    with nn.parameter_scope(\"ResNetDepth\"):\n",
        "        depth_pred = F.reshape(resnet_depth(\n",
        "            F.concatenate(inp, heatmaps, axis=1)), (68, 1))\n",
        "    pts_img = F.concatenate(\n",
        "        pts_img, depth_pred * (1.0 / (256.0 / (200.0 * scale))), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now prepare for the visualization with mplot3d. The code below is from [the original author's repository](https://github.com/1adrianb/face-alignment/blob/master/examples/detect_landmarks_in_image.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pred_type = collections.namedtuple('prediction_type', ['slice', 'color'])\n",
        "pred_types = {'face': pred_type(slice(0, 17), (0.682, 0.780, 0.909, 0.5)),\n",
        "              'eyebrow1': pred_type(slice(17, 22), (1.0, 0.498, 0.055, 0.4)),\n",
        "              'eyebrow2': pred_type(slice(22, 27), (1.0, 0.498, 0.055, 0.4)),\n",
        "              'nose': pred_type(slice(27, 31), (0.345, 0.239, 0.443, 0.4)),\n",
        "              'nostril': pred_type(slice(31, 36), (0.345, 0.239, 0.443, 0.4)),\n",
        "              'eye1': pred_type(slice(36, 42), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'eye2': pred_type(slice(42, 48), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'lips': pred_type(slice(48, 60), (0.596, 0.875, 0.541, 0.3)),\n",
        "              'teeth': pred_type(slice(60, 68), (0.596, 0.875, 0.541, 0.4))\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
      "cellView": "form"
      },
      "source": [
        "#@title Visualize the result. (double-click to see the codes)\n",
        "# uncomment the line below if you run this in jupyter notebook.\n",
        "#matplotlib notebook\n",
        "\n",
        "fig = plt.figure(figsize=plt.figaspect(.5))\n",
        "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "surf = ax.scatter(pts_img.d[:, 0] * 1.2,\n",
        "                  pts_img.d[:, 1],\n",
        "                  pts_img.d[:, 2],\n",
        "                  c='cyan',\n",
        "                  alpha=1.0,\n",
        "                  edgecolor='b')\n",
        "\n",
        "for pred_type in pred_types.values():\n",
        "    ax.plot3D(pts_img.d[pred_type.slice, 0] * 1.2,\n",
        "              pts_img.d[pred_type.slice, 1],\n",
        "              pts_img.d[pred_type.slice, 2], color='blue')\n",
        "\n",
        "ax.view_init(elev=100., azim=90.)\n",
        "ax.set_xlim(ax.get_xlim()[::-1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For Smartphone Users\n",
        "If you're using a smartphone with a camera, you can take a photo and use that image for this demo.\n",
        "Just execute the following cell and tap `Capture` button. \n",
        "\n",
        "If your device has multiple cameras (such as front and back) you need to select which one to use by tapping the corresponding button (which should appear near the 'Capture' button).\n",
        "\n",
        "**Note this is an experimental support and may not work in some devices.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    filename = take_photo(cam_width=256, cam_height=256)\n",
        "    print('Saved to {}'.format(filename))\n",
        "    # Show the image which was just taken.\n",
        "    display(Image(filename))\n",
        "except Exception as err:\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "    # grant the page permission to access it.\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the photo is OK, let's run (2D) FAN. If you want to use another photo, just re-run the previous cell again.\n",
        "\n",
        "The following cell will execute the same as done above on your photo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python model_inference.py --model 2DFAN4_NNabla_model.h5 --test-image $input_img --output output_image.png\n",
        "from IPython.display import Image,display\n",
        "display(Image('output_image.png'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
