{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67715675",
   "metadata": {},
   "source": [
    "# What is model quantization\n",
    "For most deep learning framework, 32-bit floating point is the dafault data type.\n",
    "Model quantization mean to represent the real numbers with lower bits, such as 8-bit integers.\n",
    "In this notebook, we only focus on int8 quantization.\n",
    "\n",
    "# Why we need model quantization\n",
    "+ In moving from 32-bits to 8-bits, we can reduce ths size of the model'weight by a quarter.  \n",
    "  Disk and memory usage will also be reduced.\n",
    "+ Inference with quantized model is faster than the float32 model. \n",
    "+ In some embedded devices or some Iot devices, integer operation is the only choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb503c8",
   "metadata": {},
   "source": [
    "# How to convert NNabla's nnp model to quantized tflite model\n",
    "In this section, you will learn how to convert NNabla's nnp model to INT8 quantized tflite model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6564e6d",
   "metadata": {},
   "source": [
    "* Assume that you have installed cuda and cudnn, if not, please install appropriate cuda and cudnn.  \n",
    "  See the nvidia's installation guide: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082542b",
   "metadata": {},
   "source": [
    "* Install nnabla nnabla-ext-cuda and nnabla-converter package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638a432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nnabla nnabla-ext-cuda110 nnabla-converter"
   ]
  },
  {
    "cell_type": "markdown",
    "id": "g-nvQUIS5bYE",
    "metadata": {},
    "source": [
      "* Build flatbuffers"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "id": "LYxkSo5k5kay",
    "metadata": {},
    "outputs": [],
    "source": [
      "!git clone https://github.com/google/flatbuffers.git\n",
      "!cd flatbuffers && cmake -G \"Unix Makefiles\" && make"
    ]
  },
  {
   "cell_type": "markdown",
   "id": "cebccb31",
   "metadata": {},
   "source": [
    "* Traing a simple CNN with nnabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d712528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnabla as nn\n",
    "import nnabla.functions as F\n",
    "import nnabla.parametric_functions as PF\n",
    "\n",
    "def network(image, test=False):\n",
    "    batch_state = not test\n",
    "    h = PF.convolution(image, 24, (3, 3), pad=(1,1), name='conv1')\n",
    "    h = F.relu(h)\n",
    "    x = h\n",
    "    h = PF.convolution(h, 24, (3, 3), pad=(1,1), name='conv2')\n",
    "    h = PF.batch_normalization(h, batch_stat=batch_state, name='bn1')\n",
    "    h = PF.convolution(h, 24, (3, 3), pad=(1, 1), name='conv3')\n",
    "    h = PF.batch_normalization(h, batch_stat=batch_state, name='bn2')\n",
    "\n",
    "    h = F.add2(x, h)\n",
    "    h = F.relu(h)\n",
    "\n",
    "    h = F.max_pooling(h, (2, 2))\n",
    "    h = PF.convolution(h, 32, (3, 3), pad=(1, 1), name='conv4')\n",
    "    h = PF.batch_normalization(h, batch_stat=batch_state, name='bn3')\n",
    "    h = F.relu(h)\n",
    "    x = h\n",
    "\n",
    "    h = PF.convolution(h, 32, (3, 3), pad=(1,1), name='conv5')\n",
    "    h = PF.batch_normalization(h, batch_stat=batch_state, name='bn4')\n",
    "    h = PF.convolution(h, 32, (3, 3), pad=(1, 1), name='conv6')\n",
    "    h = PF.batch_normalization(h, batch_stat=batch_state, name='bn5')\n",
    "    h = F.add2(x, h)\n",
    "    h = F.relu(h)\n",
    "\n",
    "    h = F.max_pooling(h, (2, 2))\n",
    "\n",
    "    c3 = PF.affine(h, 50, name='fc3')\n",
    "    c3 = F.relu(c3)\n",
    "    c4 = PF.affine(c3, 10, name='fc4')\n",
    "    return c4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5226bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import struct\n",
    "import zlib\n",
    "\n",
    "from nnabla.logger import logger\n",
    "from nnabla.utils.data_iterator import data_iterator\n",
    "from nnabla.utils.data_source import DataSource\n",
    "from nnabla.utils.data_source_loader import download\n",
    "\n",
    "\n",
    "def load_mnist(train=True):\n",
    "    if train:\n",
    "        image_uri = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "        label_uri = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "    else:\n",
    "        image_uri = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "        label_uri = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "    logger.info('Getting label data from {}.'.format(label_uri))\n",
    "    r = download(label_uri)\n",
    "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
    "    _, size = struct.unpack('>II', data[0:8])\n",
    "    labels = numpy.frombuffer(data[8:], numpy.uint8).reshape(-1, 1)\n",
    "    r.close()\n",
    "    logger.info('Getting label data done.')\n",
    "\n",
    "    logger.info('Getting image data from {}.'.format(image_uri))\n",
    "    r = download(image_uri)\n",
    "    data = zlib.decompress(r.read(), zlib.MAX_WBITS | 32)\n",
    "    _, size, height, width = struct.unpack('>IIII', data[0:16])\n",
    "    images = numpy.frombuffer(data[16:], numpy.uint8).reshape(\n",
    "        size, 1, height, width)\n",
    "    r.close()\n",
    "    logger.info('Getting image data done.')\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "class MnistDataSource(DataSource):\n",
    "    '''\n",
    "    Get data directly from MNIST dataset from Internet(yann.lecun.com).\n",
    "    '''\n",
    "\n",
    "    def _get_data(self, position):\n",
    "        image = self._images[self._indexes[position]]\n",
    "        label = self._labels[self._indexes[position]]\n",
    "        return (image, label)\n",
    "\n",
    "    def __init__(self, train=True, shuffle=False, rng=None):\n",
    "        super(MnistDataSource, self).__init__(shuffle=shuffle)\n",
    "        self._train = train\n",
    "\n",
    "        self._images, self._labels = load_mnist(train)\n",
    "\n",
    "        self._size = self._labels.size\n",
    "        self._variables = ('x', 'y')\n",
    "        if rng is None:\n",
    "            rng = numpy.random.RandomState(313)\n",
    "        self.rng = rng\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self._shuffle:\n",
    "            self._indexes = self.rng.permutation(self._size)\n",
    "        else:\n",
    "            self._indexes = numpy.arange(self._size)\n",
    "        super(MnistDataSource, self).reset()\n",
    "\n",
    "    @property\n",
    "    def images(self):\n",
    "        \"\"\"Get copy of whole data with a shape of (N, 1, H, W).\"\"\"\n",
    "        return self._images.copy()\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Get copy of whole label with a shape of (N, 1).\"\"\"\n",
    "        return self._labels.copy()\n",
    "\n",
    "\n",
    "def data_iterator_mnist(batch_size,\n",
    "                        train=True,\n",
    "                        rng=None,\n",
    "                        shuffle=True,\n",
    "                        with_memory_cache=False,\n",
    "                        with_file_cache=False):\n",
    "    return data_iterator(MnistDataSource(train=train, shuffle=shuffle, rng=rng),\n",
    "                         batch_size,\n",
    "                         rng,\n",
    "                         with_memory_cache,\n",
    "                         with_file_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnabla.solver as S\n",
    "import numpy as np\n",
    "\n",
    "def get_context(context, gpu_id):\n",
    "    from nnabla.ext_utils import get_extension_context\n",
    "    ctx = get_extension_context(context, device_id=gpu_id)\n",
    "    return ctx\n",
    "\n",
    "def save_model(x, y, nnpfile):\n",
    "    from nnabla.utils.save import save\n",
    "    contents = {\n",
    "        'networks': [\n",
    "            {'name': 'mnist',\n",
    "            'batch_size': 1,\n",
    "            'outputs': {'pred': y },\n",
    "            'names': {'image': x }}],\n",
    "        'executors': [\n",
    "            {'name': 'runtime',\n",
    "            'network': 'mnist',\n",
    "            'data': ['image'],\n",
    "            'output': ['pred']}]}\n",
    "    save(nnpfile, contents)\n",
    "    return\n",
    "\n",
    "def categorical_error(pred, label):\n",
    "    pred_label = pred.argmax(1)\n",
    "    return (pred_label != label.flat).mean()\n",
    "\n",
    "def train():\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "    val_interval = 1000\n",
    "    max_iter = 20000\n",
    "    val_iter = 20\n",
    "    weight_decay = 0\n",
    "    \n",
    "    image = nn.Variable([batch_size, 1, 28, 28])\n",
    "    label = nn.Variable([batch_size, 1])\n",
    "    pred = network(image)\n",
    "    pred.persistent = True\n",
    "\n",
    "    loss = F.mean(F.softmax_cross_entropy(pred, label))\n",
    "\n",
    "    vimage = nn.Variable([batch_size, 1, 28, 28])\n",
    "    vlabel = nn.Variable([batch_size, 1])\n",
    "    vpred = network(vimage, test=True)\n",
    "    solver = S.Adam(learning_rate)\n",
    "    solver.set_parameters(nn.get_parameters())\n",
    "    start_point = 0\n",
    "    from numpy.random import RandomState\n",
    "    data = data_iterator_mnist(batch_size, True, rng=RandomState(1223))\n",
    "    vdata = data_iterator_mnist(batch_size, False)\n",
    "    \n",
    "    # Training loop.\n",
    "    for i in range(start_point, max_iter):\n",
    "        if i % val_interval == 0:\n",
    "            ve = 0.0\n",
    "            for j in range(val_iter):\n",
    "                val_x, val_y = vdata.next()\n",
    "                val_x = val_x.astype(np.float32) / 255.0\n",
    "                vimage.d, vlabel.d = val_x, val_y\n",
    "                vpred.forward(clear_buffer=True)\n",
    "                vpred.data.cast(np.float32, ctx)\n",
    "                ve += categorical_error(vpred.d, vlabel.d)\n",
    "            print(\"Step:{}  Val Error:{}\".format(i, ve/val_iter))\n",
    "\n",
    "        train_x, train_y = data.next()\n",
    "        train_x = train_x.astype(np.float32) / 255.0\n",
    "        image.d, label.d = train_x, train_y\n",
    "\n",
    "        solver.zero_grad()\n",
    "        loss.forward(clear_no_need_grad=True)\n",
    "        loss.backward(clear_buffer=True)\n",
    "        solver.weight_decay(weight_decay)\n",
    "        solver.update()\n",
    "        loss.data.cast(np.float32, ctx)\n",
    "        pred.data.cast(np.float32, ctx)\n",
    "        e = categorical_error(pred.d, label.d)\n",
    "\n",
    "    ve = 0.0\n",
    "    for j in range(val_iter):\n",
    "        vimage.d, vlabel.d = data.next()\n",
    "        vpred.forward(clear_buffer=True)\n",
    "        ve += categorical_error(vpred.d, vlabel.d)\n",
    "    \n",
    "    # Collect represent dataset for quantization\n",
    "    represent_dataset = []\n",
    "    for j in range(20): # 20 * 128\n",
    "        x, _ = data.next()\n",
    "        represent_dataset.append(x.astype(np.float32) / 255.0)\n",
    "    represent_dataset = np.concatenate(represent_dataset, axis=0)\n",
    "    # Save represent dataset and nnp model\n",
    "    np.save('mnist.npy', represent_dataset)\n",
    "    save_model(vimage, vpred, 'mnist.nnp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = get_context('cudnn', '0')\n",
    "nn.set_default_context(ctx)\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f02794",
   "metadata": {},
   "source": [
    "* convert nnp model to tflite format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nnp to float32 tflite\n",
    "!PATH=$PATH:$(pwd)/flatbuffers nnabla_cli convert -b 1 mnist.nnp mnist.tflite\n",
    "# convert nnp to int8 tflite\n",
    "!PATH=$PATH:$(pwd)/flatbuffers nnabla_cli convert -b 1 mnist.nnp mnist_int8.tflite --quantization --dataset mnist.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb06937",
   "metadata": {},
   "source": [
    "# Evaluate the tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def evaluate_model(tflite, test_images, test_labels):\n",
    "    interpreter = tf.lite.Interpreter(tflite)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=i))\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "        test_image = np.transpose(test_image, (0,2,3,1))\n",
    "\n",
    "        # Quantize input\n",
    "        if input_details['dtype'] == np.int8 or input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        test_image = test_image.astype(input_details['dtype'])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.tensor(output_index)()[0]\n",
    "        output = output.astype(np.float32)\n",
    "    \n",
    "        # Dequantize output\n",
    "        if output_details['dtype'] == np.int8:\n",
    "            output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "            output = (output - output_zero_point) * output_scale\n",
    "        digit = np.argmax(output)\n",
    "        prediction_digits.append(digit)\n",
    "\n",
    "    print('\\n')\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ee608",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = load_mnist(False)\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# test with float32 tflite\n",
    "tflite_weight = './mnist.tflite'\n",
    "acc = evaluate_model(tflite_weight, test_images, test_labels[...,0])\n",
    "print(\"accuracy of {} is: {}\\n\".format(tflite_weight, acc))\n",
    "\n",
    "# test with int8 tflite\n",
    "int8_tflite_weight = './mnist_int8.tflite'\n",
    "acc = evaluate_model(int8_tflite_weight, test_images, test_labels[...,0])\n",
    "print(\"accuracy of {} is: {}\\n\".format(int8_tflite_weight, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f263bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l -h {tflite_weight}\n",
    "!ls -l -h {int8_tflite_weight}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942a8b2",
   "metadata": {},
   "source": [
    "We can see the accuracy of int8 quantized tflite model and the accuracy of float32 model are very close! But the size of int8 quantized tflite is about a quarter of the float32 model.  \n",
    "Now, just try this converter with your own model.  \n",
    "Note that not all of NNabla functions can be converted to quantized tflite op.\n",
    "You can check this page for more details: https://nnabla.readthedocs.io/en/latest/python/file_format_converter/INT8_TFLite_Support_Status.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
