{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Example\n",
    "\n",
    "Floating-point number for representing weight coefficients is normally used \n",
    "in deep learning both in training and inference. However, 32-bits might be too \n",
    "representative for weight coefficients since a neural network has the number of parameters \n",
    "enough to solve a problem at one's hand, thus much more fewer bits, for example in the \n",
    "extreme case, 1 bit, might be enough. \n",
    "\n",
    "In the end of 2015, two papers were published.\n",
    "\n",
    "```\n",
    "1. Song Han, Huizi Mao, William J. Dally,\n",
    "\"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\",\n",
    "https://arxiv.org/abs/1510.00149\n",
    "\n",
    "2. Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David, \n",
    "\"BinaryConnect: Training Deep Neural Networks with binary weights during propagations\",\n",
    "https://arxiv.org/abs/1511.00363\n",
    "\n",
    "```\n",
    "\n",
    "In the first paper, they showed that neural networks are compressed deeply, and  \n",
    "one can get a network with much fewer parameters. Namely, in the second paper, \n",
    "they showed that binary weight (+1/-1) is enough to train neural networks,\n",
    "in addition, activations can also be signed, which may accelerate the inference\n",
    "speed.\n",
    "\n",
    "These works have changed our mind for training neural networks completely in a way \n",
    "that one can train a neural network in a fewer bits. and hereafter\n",
    "there have been a lot of papers published about quantized neural networks.\n",
    "\n",
    "In this quantization example, we show a family of quantized neural networks. \n",
    "`models.py` contains the following quantized neural networks, \n",
    "\n",
    "\n",
    "1. binary connect\n",
    "2. binary net\n",
    "3. binary weight\n",
    "4. fixed-point (uniform) connect\n",
    "5. fixed-point (uniform) net\n",
    "6. pow2 connect\n",
    "7. pow2 net\n",
    "8. incremental network quantization\n",
    "9. min-max quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using this example, run the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python classification.py -c \"cudnn\" \\\n",
    "    --net cifar10_fp_connect_resnet23_prediction \\\n",
    "    --monitor-path \"monitor.fpcon\" \\\n",
    "    --model-save-path \"monitor.fpcon\" \\\n",
    "    -d 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Courbariaux Matthieu, Bengio Yoshua, David Jean-Pierre, \"BinaryConnect: Training Deep Neural Networks with binary weights during propagations\", Advances in Neural Information Processing Systems 28 (NIPS 2015)\n",
    "2. Rastegari Mohammad, Ordonez Vicente, Redmon Joseph, and Farhadi Ali, \"XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\", arXiv:1603.05279\n",
    "3. Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen, \"Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights\", arXiv:1702.03044\n",
    "4. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko, \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\", https://arxiv.org/abs/1712.05877"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
