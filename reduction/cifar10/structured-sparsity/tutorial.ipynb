{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Sparsity Example\n",
    "\n",
    "It is well-known that neural networks have redundant filters, thus one would like\n",
    "to reduce the number of filters, then the number of output feature maps \n",
    "in convolutions and the number of output dimensions in affine are reduced, \n",
    "which not only reduces the memory space but also the computational cost. \n",
    "\n",
    "For example, in 2D Convolution case, one can get a slim network by reducing unnecessary 3D kernels, \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$w_m \\in \\mathcal{R}^{N \\times K_h \\times K_w}$ where \n",
    "$w_m$ denotes one convolution 3d filter, \n",
    "$N$ is the number of input maps, \n",
    "$K_h$ is the kernel height, and \n",
    "$K_w$ is the kernel width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be achieved by sparsing the filters. It is induced by using \n",
    "`Structured Sparsity Learning` called `SSL` in the following paper, \n",
    "\n",
    "```\n",
    "WeiWen, et al., \n",
    "\"Learning Structured Sparsity in Deep Neural Networks\",\n",
    "https://arxiv.org/abs/1608.03665\n",
    "```\n",
    "\n",
    "Literally, `SSL` includes the filters which have many zeros elements but it has structure, \n",
    "in this case, $w_m$ might become zero, thus one can ignore such filters. Thus,\n",
    "one get a slim network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, there are two regularization to induce sparsity; $R_f(W)$ and $R_c(W)$, Each of which are denoted by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R_f(W) = \\sum_{m=1}^{M}\\sqrt{\\sum_{n,k_{h},k_{w}=1}^{N,K_{h},K_{w}}w_{m,n,k_{h}, k{w}}^{2}},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R_c(W) = \\sum_{n=1}^{N}\\sqrt{\\sum_{m,k_{h},k_{w}=1}^{M,K_{h},K_{w}}w_{m,n,k_{h}, k{w}}^{2}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $R_f(W)$ induces the *filter-wise* sparsity and $R_c(W)$ does the *channel-wise* sparsity. Note that $R_c(W)$ also induces the *filter-wise* sparsity since in Neural Network context, an input map is the result of a preceding layer so that including the *channel-wise* sparsity corresponds to the *filter-wise* sparsity in the preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, $R_f(W)$ and $R_c(W)$ are used together for each layer, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\lambda_f \\sum_{l=1}^{L} R_f(W^{l}) + \\lambda_c \\sum_{l=1}^{L} R_c(W^{l}),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda_f$ and $\\lambda_c$ are the hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One follows the steps for using `SSL`,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a reference network with Structured Sparsity induced regularization\n",
    "2. Finetune a reference network without unnecessary filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using this example, first train a network,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python classification.py -c \"cudnn\" \\\n",
    "    --monitor-path \"monitor.filter.lambda-5e4\" \\\n",
    "    --model-save-path \"monitor.filter.lambda-5e4\" \\\n",
    "    --filter-decay 5e-4 \\\n",
    "    --channel-decay 5e-4 \\\n",
    "    -d 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, finetune that network,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "\n",
    "python finetuning.py -c \"cudnn\" \\\n",
    "    --monitor-path \"monitor.finetune.filter.lambda-5e4.rrate-025\" \\\n",
    "    --model-save-path \"monitor.finetune.filter.lambda-5e4.rrate-025\" \\\n",
    "    --model-load-path \"monitor.filter.lambda-5e4/${the best result}.h5\" \\\n",
    "    --reduction-rate 0.25 \\\n",
    "    --val-interval 1000 \\\n",
    "    -d 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Wen Wei, Wu Chunpeng, Wang Yandan, Chen Yiran, and Li Hai \"Learning Structured Sparsity in Deep Neural Networks\", arXiv:1608.03665"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
