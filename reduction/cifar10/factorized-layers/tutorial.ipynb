{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Example\n",
    "\n",
    "Large convolution and affine layers can be decomposed into smaller layers to reduce the number\n",
    "of parameters and computations. \n",
    "NNabla introduces a set of factorized layer types which can approximate the functions of \n",
    "larger convolution or affine layers. The parameters of the factorized layers can be initialized \n",
    "by low rank approximation of the original layers. The factorized layers currently include:\n",
    "\n",
    "## SVD Affine\n",
    "\n",
    "SVD affine is a low rank approximation of the affine layer. It can be seen as \n",
    "two consecutive affine layers with a bottleneck.\n",
    "It computes $\\mathbf{y} = \\mathbf{U} \\mathbf{V} \\mathbf{x} + \\mathbf{b}$, where $\\mathbf{x}$\n",
    "and $\\mathbf{y}$ are the inputs and outputs respectively,and $\\mathbf{U}$, $\\mathbf {V}$ and \n",
    "$\\mathbf{b}$ are constants.\n",
    "\n",
    "The weights $\\mathbf{U}$ and $\\mathbf{V}$ are approximated with singular value decomposition (SVD) \n",
    "of the original weight matrix $\\mathbf{W}$ and by selecting the $R$ dominant singular \n",
    "values and the corresponding singular vectors. Therefore the low rank $R$ is the size \n",
    "of the bottleneck. \n",
    "\n",
    "## SVD Convolution\n",
    "  \n",
    "SVD convolution is a low rank approximation of the convolution layer. It can\n",
    "be seen as a depth wise convolution followed by a $1\\times1$ convolution. \n",
    "The flattened kernels for the $i^{th}$ input map are expressed by their low rank approximation.\n",
    "The kernels for the $i^{th}$ input $\\mathbf{W}_i$ are approximated with the singular value \n",
    "decomposition (SVD) and by selecting the $R$ dominant singular values and the\n",
    "corresponding singular vectors. \n",
    "  \n",
    "  $$ \\mathbf{W}_{:,i,:} \\approx \\mathbf{U}_i \\mathbf{V}_i $$\n",
    "\n",
    "$\\mathbf{U}$ contains the weights of the depthwise convolution with multiplier $R$ and \n",
    "$\\mathbf{V}$ contains the weights of the $1\\times1$ convolution. \n",
    "  \n",
    "Note that if $R=1$ the structure is equivalent to the depthwise separable convolution introduced in [1]\n",
    "\n",
    "\n",
    "## CP Convolution\n",
    "\n",
    "CP convolution is a low rank approximation of the 3D kernel tensor of a convolution layer. It \n",
    "can be seen as linear combinations of the input feature maps to $R$ feature maps followed\n",
    "by a depthwise convolution and followed by linear combinations of the feature maps to compute the\n",
    "output feature maps. \n",
    "The CP decomposition allows to approximate the kernel tensor by $R$ rank-$1$ tensors of the form:\n",
    "  \n",
    "  $$ \\sum_{r=1}^{R} \\lambda_r \\mathbf{o}^{(r)}\\otimes\\mathbf{i}^{(r)}\\otimes\\mathbf{k}^{(r)} $$\n",
    "  \n",
    "where $\\lambda$ is the normalization coefficient and $\\otimes$ is the outer product.\n",
    "  \n",
    "CP layers were introduced in [2], however they decompose the $4$D weight tensor while, here, the \n",
    "CP convolution is initialized to approximate the $3$D weight tensor with reshaped kernels as in [3].\n",
    "\n",
    "In this example, we show how we can reduce the size of a neural network using the factorized layers\n",
    "initialized from the layers of a pre-trained network on CIFAR10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example can be ran as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the original network first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python classification.py -o './original_net' \\\n",
    "                         -d 0  -c cudnn \\\n",
    "                         --net 'cifar10_resnet23_prediction'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the reduced network with SVD affine and SVD convolutions (weight compression rate is at least 40%):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python classification.py -o './svd_net' \\\n",
    "                         -d 0  -c cudnn \\\n",
    "                         --model-load-path './original_net/params_224000.h5' \\\n",
    "                         --net 'cifar10_svd_factorized_resnet23_prediction' \\\n",
    "                         --compression_ratio 0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use CP convolution, \n",
    "train the reduced network with SVD affine and CP convolutions (weight compression rate is at least 40%):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "python classification.py -o './cp_net' \\\n",
    "                         -d 0  -c cudnn \\\n",
    "                         --model-load-path './original_net/params_2240000.h5' \\\n",
    "                         --net 'cifar10_cpd3_factorized_resnet23_prediction' \\\n",
    "                         --compression_ratio 0.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\n",
    "Marco Andreetto, and Hartwig Adam.  \n",
    "\"Mobilenets: Efficient convolutional neural networks for mobile vision applications.\"\n",
    "https://arxiv.org/pdf/1704.04861\n",
    "\n",
    "2. Lebedev, Vadim, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky. \n",
    "\"Speeding-up convolutional neural networks using fine-tuned cp-decomposition.\" \n",
    "arXiv preprint arXiv:1412.6553 (2014).\n",
    "\n",
    "3. Astrid, Marcella, and Seung-Ik Lee. \n",
    "\"CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression.\" \n",
    "In Big Data and Smart Computing (BigComp), \n",
    "2017 IEEE International Conference on, pp. 115-118. IEEE, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
